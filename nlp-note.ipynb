{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/FudanNLP/nlp-beginner 参考邱锡鹏老师的task\n",
    "\n",
    "https://nndl.github.io/nndl-book.pdf 《神经网络与深度学习》邱锡鹏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#线性回归\n",
    "#tensorflow做梯度下降回归\n",
    "import tensorflow.compat.v1 as tf#tensorflow1.0版本\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "class LrModel():\n",
    "    def __init__(self, config, seq_length):\n",
    "        self.config = config\n",
    "        self.seq_length = seq_length\n",
    "        self.lr()\n",
    "\n",
    "    def lr(self):\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.seq_length])\n",
    "        w = tf.Variable(tf.zeros([self.seq_length, self.config.num_classes]))\n",
    "        b = tf.Variable(tf.zeros([self.config.num_classes]))\n",
    "\n",
    "        y = (tf.matmul(self.x, w) + b)\n",
    "\n",
    "        self.y_pred_cls = tf.argmax(y, 1)\n",
    "\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, self.config.num_classes])\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(self.y_ * tf.log(y), reduction_indices=[1]))\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(self.y_, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类任务：\n",
    "\n",
    "数据集用 $X$ 表示,$X = {(x^1 , y^1 ), ... , (x^N , y^N )}$,N 为 样本个数。$y^i$ 为样本实例 $x^i$ 的类别标记。在自然语言处理中,数据集也经常称为语料库。\n",
    "\n",
    "## 向量化:\n",
    "在机器学习中,为了更好地表示样本的属性,一般将样本表示成代数形式,称为样本的特征,我们用$\\phi(x)$。样本特征可以是一维或多维向量,$\\phi(x) ∈ R_m,m$是向量维数。 $$ \\phi(x) = \\left( \\begin{array} {ccc} \\phi_1(x) \\ \\phi_2 (x) \\ .\\ .\\ \\phi_m(x) \\end{array} \\right) $$自然语言处理中,数据都是以字符形式存在的。\n",
    "样本的原始表示一般是字符串序列。为了便于使用机器学习方法,首先要把样本表示为向量形式。下面我们介绍几种常 用的特征表示方法。自然语言处理的,在构造了样本和样本集合之后,为了和后面的机 器学习算法相结合,我们将样本x 转变成向量φ(x)。在将字符表示转换成向量表示的过 程中需要很多中间步骤,我们把这些中间步骤都成为数据处理,并且尽可能的模块化。\n",
    ">> 2.1 词袋模型\n",
    "一种简单的方法是简单假设文本(如一个句子或一个文档)是由字、词组成的无序多重集合,不考虑语法甚至词序。这就是在自然语言处理和信息检索中常用的词袋模型，词袋模型可以看成一种以词为基本单位的向量空间模型(Vector Space Model, VSM)。具体可见本课程chap3的slide\n",
    "\n",
    ">>2.2 N 元特征\n",
    "词袋模型在需要深层分析的场合就会显得太过简化了。例如在语义分析里,“你打了 我”和“我打了你”,意思是相反的,但用词袋模型表示后,这两句话是向量表示的等价 的,这显然是不合理的。\n",
    "\n",
    ">>N 元特征(N-gram 特征),顾名思义,就是由 N 个字或词组成的字符串,单元可以 是字或词。这里N是大于等于1的任意整数。如果N 为2,就叫做二元特征,如果N为 3,就叫做三元特征以此类推。\n",
    "\n",
    ">>N 元特征可以看出是对词袋模型的一种改进方法。与 N 元特征相关的概念是 N 元语法模型。以中文句子“机器学习算法”为例,以字为基本单位的二元特征集合为:{机器,器 学,学习,习算,算法}。集合中每一项都是由二个相邻的字组成的的子串,长度为 2。这 些子串可以是有意义的词(例如:“学习”、“算法”),也可以是无任何意义的字符串(例 如:“器学”,“习算”)。但是这些无意义的子串也有可能在分类中起到很关键的作用。一 个长度为L的句子,可以提取出L − 1个二元特征。\n",
    "\n",
    ">>有了 N 元特征集合,就可以利用词袋模型将文本表示为向量形式。随着 N 的增加, 可以抽取的特征就会越多,特征空间也会呈指数增加。这些高阶的特征出现的频率也会相对较低,对分类不但没有太多帮助,还会直接影响着后续处理的效率与复杂度。因此在一般的文本分类任务中,N 取 3 就足够了,并且同时也使用一元和二元特征,防止出现过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类的做法\n",
    "> 经过特征抽取后,一个样本可以表示为 k 维特征空间中的一个点。为了对这个特征空间中的点进行区分,就需要寻找一些超平面来将这个特征空间分为一些互不重叠的子区域,使得不同类别的点分布在不同的子区域中,这些超平面就成为判别界面。\n",
    "为了定义这些用来进行空间分割的超平面,就需要引入判别函数的概念。假设变量 $z ∈ R_m$为特征空间中的点,这个超平面由所有满足函数f(z) = 0的点组成。这里的f(z)就称为判别函数。\n",
    "有了判别函数,分类就变得很简单,就是看一个样本在特征空间中位于哪个区域, 从而确定这个样本的类别. 判别函数的形式多种多样,在自然语言处理中,最为常用的判别函数为线性函数。\n",
    "## 二分类\n",
    "二分类问题\n",
    "$$ \\hat y =sign((f(z))) = sign(\\theta^Tz+\\theta_0) $$\n",
    "\n",
    "用这里的正负号判断\n",
    "\n",
    "判别函数为 $$ f(z) ＝ \\theta^Tz+\\theta_0 = \\sum_{i=1}^{k}\\theta_iz_i + \\theta_0 = \\sum_{i=0}^{k} = \\hat \\theta^T \\hat z $$ 其中$z_0=1$,$\\hat\\theta,\\hat z$分别称为增广权重向量和增广特征向量。 $$ \\hat z = \\left( \\begin{array} {ccc} 1 \\ z_1\\ .\\ .\\ z_k \\end{array} \\right) = \\left( \\begin{array}{ccc} 1 \\ \\ z \\ \\ \\\n",
    "\\end{array} \\right) $$\n",
    "\n",
    "$$ \\hat \\theta = \\left(\\begin{array} {ccc} \\theta_0 \\ \\theta_1\\ .\\ .\\ \\theta_k \\end{array} \\right) = \\left(\\begin{array}{ccc}\\theta_0 \\ \\ \\theta \\ \\ \\\n",
    "\\end{array} \\right) $$\n",
    "\n",
    "可以直接用线性回归来进行二分类\n",
    "\n",
    "在更大数据量下，梯度下降会更好，比最小二乘法更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef run():\\n    learning_rate = 0.0001\\n    initial_b = random()\\n    initial_w = random()\\n    num_iterations = 1000\\n    print(\"Starting gradient descent at b = {0}, w = {1}, error = {2}\"\\n          .format(initial_b, initial_w, \\n                  compute_error_for_line_given_points(initial_b, initial_w, points)))\\n    print(\"Running...\")\\n    [b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)\\n    print(\"After {0} iterations at b = {1}, w = {2}, error = {3}\"\\n          .format(num_iterations, b, w, \\n                  compute_error_for_line_given_points(b, w, points)))\\nrun()\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用numpy来进行梯度下降线性回归\n",
    "import numpy as np\n",
    "def compute_error_for_line_given_points(b, w, points):\n",
    "    totalError = 0\n",
    "    for i in range(len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (w * x + b)) ** 2\n",
    "    return totalError / float(len(points)) # average\n",
    "def step_gradient(b_current, w_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    w_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += 2 * ((w_current * x) + b_current - y)\n",
    "        w_gradient += 2 * x * ((w_current * x) + b_current - y)\n",
    "    b_gradient = b_gradient / N\n",
    "    w_gradient = w_gradient / N\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_w = w_current - (learningRate * w_gradient)\n",
    "    return [new_b, new_w]\n",
    "def gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations): # num_iteration 迭代次数\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_gradient(b, w, np.array(points), learning_rate)\n",
    "    return [b, w]\n",
    "\"\"\"\n",
    "def run():\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = random()\n",
    "    initial_w = random()\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, w = {1}, error = {2}\"\n",
    "          .format(initial_b, initial_w, \n",
    "                  compute_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)\n",
    "    print(\"After {0} iterations at b = {1}, w = {2}, error = {3}\"\n",
    "          .format(num_iterations, b, w, \n",
    "                  compute_error_for_line_given_points(b, w, points)))\n",
    "run()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上课笔记\n",
    "\n",
    "logistic回归\n",
    "$$y=\\frac{1}{1+e^{-(\\omega^T+b)}}$$\n",
    "\n",
    "sigmoid函数 `性质`：函数输出作为概率值;特征加权累加；非线性变化\n",
    "\n",
    "· x为正例的概率越大，几率取值即越大\n",
    "· 输出值越接近1说明输入数据x分类为该类别的可能性大\n",
    "参数求取，损失函数最小化，对数似然的相反数：即交叉熵\n",
    "$$T(\\theta)=log(L(\\theta|D))=-\\sum{n}{i=1}(y_{i}log(h_{\\theta}(x_{i}))+(1-y_{i}log(1-h_{\\theta}(x_{i}))$$\n",
    "\n",
    "从回归到softmax分类：二分类到多分类\n",
    "\n",
    "\n",
    "矩阵分解的潜在语义分析\n",
    "奇异值分解：将一个矩阵分解为两个正将矩阵和一个对角矩阵的乘积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.externals import joblib\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "config = LrConfig()\n",
    "\n",
    "\n",
    "class DataProcess():\n",
    "    def __init__(self, dataset_path=None, stopwords_path=None, model_save_path=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.stopwords_path = stopwords_path\n",
    "        self.model_save_path = model_save_path\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        stopwords = list()\n",
    "        with open(self.dataset_path, encoding='utf-8') as f1:\n",
    "            data = f1.readlines()\n",
    "        with open(self.stopwords_path, encoding='utf-8') as f2:\n",
    "            temp_stopwords = f2.readlines()\n",
    "        for word in temp_stopwords:\n",
    "            stopwords.append(word[:-1])\n",
    "        return data, stopwords\n",
    "\n",
    "    def save_categories(self, data, save_path):\n",
    "        \"\"\"将文本的类别写到本地\"\"\"\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('|'.join(data))\n",
    "\n",
    "    def pre_data(self, data, stopwords, test_size=0.2):\n",
    "        \"\"\"数据预处理\"\"\"\n",
    "        label_list = list()\n",
    "        text_list = list()\n",
    "        for line in data:\n",
    "            label, text = line.split('\\t', 1)\n",
    "            # print(label)\n",
    "            seg_text = [word for word in jieba.cut(text) if word not in stopwords]\n",
    "            text_list.append(' '.join(seg_text))\n",
    "            label_list.append(label)\n",
    "        # 标签转化为one-hot格式\n",
    "        encoder_nums = LabelEncoder()\n",
    "        label_nums = encoder_nums.fit_transform(label_list)\n",
    "        categories = list(encoder_nums.classes_)\n",
    "        self.save_categories(categories, config.categories_save_path)\n",
    "        label_nums = np.array([label_nums]).T\n",
    "        encoder_one_hot = OneHotEncoder()\n",
    "        label_one_hot = encoder_one_hot.fit_transform(label_nums)\n",
    "        label_one_hot = label_one_hot.toarray()\n",
    "        return model_selection.train_test_split(text_list, label_one_hot, test_size=test_size, random_state=1024)\n",
    "\n",
    "    # TODO:后续做\n",
    "    def get_bow(self):\n",
    "        \"\"\"提取词袋模型特征\"\"\"\n",
    "        pass\n",
    "\n",
    "    # TODO:这里可能出现维度过大，内存不足的问题，目前是去除低频词解决，可以做lda或者pca降维（后续做）\n",
    "    def get_tfidf(self, X_train, X_test):\n",
    "        \"\"\"提取tfidf特征\"\"\"\n",
    "        vectorizer = TfidfVectorizer(min_df=100)\n",
    "        vectorizer.fit_transform(X_train)\n",
    "        X_train_vec = vectorizer.transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "        return X_train_vec, X_test_vec, vectorizer\n",
    "\n",
    "    # TODO:后续做\n",
    "    def get_word2vec(self):\n",
    "        \"\"\"提取word2vec特征\"\"\"\n",
    "        pass\n",
    "\n",
    "    def provide_data(self):\n",
    "        \"\"\"提供数据\"\"\"\n",
    "        data, stopwords = self.read_data()\n",
    "        #  1、提取bag of word参数\n",
    "        #  2、提取tf-idf特征参数\n",
    "        X_train, X_test, y_train, y_test = self.pre_data(data, stopwords, test_size=0.2)\n",
    "        X_train_vec, X_test_vec, vectorizer = self.get_tfidf(X_train, X_test)\n",
    "        joblib.dump(vectorizer, self.model_save_path)\n",
    "        #  3、提取word2vec特征参数\n",
    "        return X_train_vec, X_test_vec, y_train, y_test\n",
    "\n",
    "    def batch_iter(self, x, y, batch_size=64):\n",
    "        \"\"\"迭代器，将数据分批传给模型\"\"\"\n",
    "        data_len = len(x)\n",
    "        num_batch = int((data_len-1)/batch_size)+1\n",
    "        indices = np.random.permutation(np.arange(data_len))\n",
    "        x_shuffle = x[indices]\n",
    "        y_shuffle = y[indices]\n",
    "        for i in range(num_batch):\n",
    "            start_id = i*batch_size\n",
    "            end_id = min((i+1)*batch_size, data_len)\n",
    "            yield x_shuffle[start_id: end_id], y_shuffle[start_id: end_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('G:/download/train/train.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         A series of escapades demonstrating the adage ...\n",
       "1         A series of escapades demonstrating the adage ...\n",
       "2                                                  A series\n",
       "3                                                         A\n",
       "4                                                    series\n",
       "5         of escapades demonstrating the adage that what...\n",
       "6                                                        of\n",
       "7         escapades demonstrating the adage that what is...\n",
       "8                                                 escapades\n",
       "9         demonstrating the adage that what is good for ...\n",
       "10                                  demonstrating the adage\n",
       "11                                            demonstrating\n",
       "12                                                the adage\n",
       "13                                                      the\n",
       "14                                                    adage\n",
       "15                          that what is good for the goose\n",
       "16                                                     that\n",
       "17                               what is good for the goose\n",
       "18                                                     what\n",
       "19                                    is good for the goose\n",
       "20                                                       is\n",
       "21                                       good for the goose\n",
       "22                                                     good\n",
       "23                                            for the goose\n",
       "24                                                      for\n",
       "25                                                the goose\n",
       "26                                                    goose\n",
       "27        is also good for the gander , some of which oc...\n",
       "28        is also good for the gander , some of which oc...\n",
       "29                                                  is also\n",
       "                                ...                        \n",
       "156030                          a joke in the United States\n",
       "156031    The movie 's downfall is to substitute plot fo...\n",
       "156032                                The movie 's downfall\n",
       "156033              is to substitute plot for personality .\n",
       "156034                is to substitute plot for personality\n",
       "156035                   to substitute plot for personality\n",
       "156036                      substitute plot for personality\n",
       "156037                                      substitute plot\n",
       "156038                                      for personality\n",
       "156039    The film is darkly atmospheric , with Herrmann...\n",
       "156040    is darkly atmospheric , with Herrmann quietly ...\n",
       "156041    is darkly atmospheric , with Herrmann quietly ...\n",
       "156042                              is darkly atmospheric ,\n",
       "156043                                is darkly atmospheric\n",
       "156044    with Herrmann quietly suggesting the sadness a...\n",
       "156045    Herrmann quietly suggesting the sadness and ob...\n",
       "156046                                             Herrmann\n",
       "156047    quietly suggesting the sadness and obsession b...\n",
       "156048    suggesting the sadness and obsession beneath H...\n",
       "156049                 suggesting the sadness and obsession\n",
       "156050                            the sadness and obsession\n",
       "156051                                sadness and obsession\n",
       "156052                                          sadness and\n",
       "156053          beneath Hearst 's forced avuncular chortles\n",
       "156054                  Hearst 's forced avuncular chortles\n",
       "156055                                            Hearst 's\n",
       "156056                            forced avuncular chortles\n",
       "156057                                   avuncular chortles\n",
       "156058                                            avuncular\n",
       "156059                                             chortles\n",
       "Name: Phrase, Length: 156060, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Train=train['Phrase']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "vectorizer = TfidfVectorizer(min_df=100)\n",
    "a=vectorizer.fit_transform(_Train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_h=OneHotEncoder()\n",
    "sentiment=one_h.fit_transform(spres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=_Train.iloc[:2000]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec=vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100',\n",
       " '13',\n",
       " '25',\n",
       " '60',\n",
       " 'aborted',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolute',\n",
       " 'absorbing',\n",
       " 'accents',\n",
       " 'across',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actually',\n",
       " 'adage',\n",
       " 'after',\n",
       " 'against',\n",
       " 'age',\n",
       " 'agers',\n",
       " 'aggressive',\n",
       " 'aims',\n",
       " 'airless',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'also',\n",
       " 'ambition',\n",
       " 'amounts',\n",
       " 'amuses',\n",
       " 'amusing',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'apart',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arthur',\n",
       " 'arts',\n",
       " 'as',\n",
       " 'assault',\n",
       " 'at',\n",
       " 'attempts',\n",
       " 'avengers',\n",
       " 'avoid',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'banality',\n",
       " 'bartlett',\n",
       " 'baseball',\n",
       " 'be',\n",
       " 'becomes',\n",
       " 'being',\n",
       " 'beneath',\n",
       " 'best',\n",
       " 'betrayal',\n",
       " 'better',\n",
       " 'beyond',\n",
       " 'big',\n",
       " 'bilingual',\n",
       " 'bind',\n",
       " 'blame',\n",
       " 'bloody',\n",
       " 'boilerplate',\n",
       " 'both',\n",
       " 'boy',\n",
       " 'but',\n",
       " 'by',\n",
       " 'bygone',\n",
       " 'call',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'cartoon',\n",
       " 'case',\n",
       " 'cattaneo',\n",
       " 'celebrated',\n",
       " 'chabrol',\n",
       " 'chance',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'characters',\n",
       " 'charm',\n",
       " 'charmer',\n",
       " 'childhood',\n",
       " 'chuck',\n",
       " 'chuckles',\n",
       " 'cinematic',\n",
       " 'claude',\n",
       " 'clearly',\n",
       " 'clever',\n",
       " 'cliche',\n",
       " 'cliched',\n",
       " 'cliches',\n",
       " 'clips',\n",
       " 'clunky',\n",
       " 'combination',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'common',\n",
       " 'companion',\n",
       " 'complications',\n",
       " 'computer',\n",
       " 'considerable',\n",
       " 'considers',\n",
       " 'constructed',\n",
       " 'converted',\n",
       " 'convolutions',\n",
       " 'cooler',\n",
       " 'could',\n",
       " 'couple',\n",
       " 'couples',\n",
       " 'cradles',\n",
       " 'credits',\n",
       " 'crime',\n",
       " 'crisis',\n",
       " 'damned',\n",
       " 'dark',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'david',\n",
       " 'day',\n",
       " 'deceit',\n",
       " 'decent',\n",
       " 'deconstruction',\n",
       " 'deep',\n",
       " 'deepest',\n",
       " 'deeply',\n",
       " 'defend',\n",
       " 'delivered',\n",
       " 'demonstrating',\n",
       " 'designed',\n",
       " 'despite',\n",
       " 'detailing',\n",
       " 'developments',\n",
       " 'dialogue',\n",
       " 'different',\n",
       " 'directions',\n",
       " 'director',\n",
       " 'disagree',\n",
       " 'distort',\n",
       " 'distractions',\n",
       " 'disturb',\n",
       " 'dizzily',\n",
       " 'do',\n",
       " 'documentary',\n",
       " 'does',\n",
       " 'dogs',\n",
       " 'down',\n",
       " 'downright',\n",
       " 'drama',\n",
       " 'drawn',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drinker',\n",
       " 'drive',\n",
       " 'during',\n",
       " 'early',\n",
       " 'earnest',\n",
       " 'easily',\n",
       " 'embarrassingly',\n",
       " 'endless',\n",
       " 'ends',\n",
       " 'engaging',\n",
       " 'entertaining',\n",
       " 'epic',\n",
       " 'era',\n",
       " 'eric',\n",
       " 'error',\n",
       " 'escapades',\n",
       " 'escapism',\n",
       " 'ethnography',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everything',\n",
       " 'evil',\n",
       " 'ex',\n",
       " 'exclusively',\n",
       " 'exercise',\n",
       " 'exists',\n",
       " 'expect',\n",
       " 'experimental',\n",
       " 'extravagant',\n",
       " 'fallon',\n",
       " 'familiar',\n",
       " 'fans',\n",
       " 'fantasia',\n",
       " 'far',\n",
       " 'fatal',\n",
       " 'fears',\n",
       " 'feature',\n",
       " 'feel',\n",
       " 'feels',\n",
       " 'fi',\n",
       " 'film',\n",
       " 'filmmaker',\n",
       " 'films',\n",
       " 'finally',\n",
       " 'finish',\n",
       " 'first',\n",
       " 'fisted',\n",
       " 'five',\n",
       " 'flick',\n",
       " 'flickering',\n",
       " 'foibles',\n",
       " 'followed',\n",
       " 'for',\n",
       " 'forces',\n",
       " 'forth',\n",
       " 'fortunately',\n",
       " 'freakish',\n",
       " 'french',\n",
       " 'fresnadillo',\n",
       " 'frighten',\n",
       " 'from',\n",
       " 'frothing',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'furiously',\n",
       " 'gag',\n",
       " 'gallo',\n",
       " 'gambles',\n",
       " 'games',\n",
       " 'gander',\n",
       " 'garner',\n",
       " 'gay',\n",
       " 'generated',\n",
       " 'generic',\n",
       " 'gently',\n",
       " 'gets',\n",
       " 'girlfriend',\n",
       " 'give',\n",
       " 'gives',\n",
       " 'glacial',\n",
       " 'glorification',\n",
       " 'goal',\n",
       " 'going',\n",
       " 'good',\n",
       " 'goose',\n",
       " 'gorgeous',\n",
       " 'grace',\n",
       " 'gratuitous',\n",
       " 'grenade',\n",
       " 'gritty',\n",
       " 'hack',\n",
       " 'ham',\n",
       " 'hampered',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'hate',\n",
       " 'hatfield',\n",
       " 'have',\n",
       " 'hell',\n",
       " 'here',\n",
       " 'heroes',\n",
       " 'hicks',\n",
       " 'high',\n",
       " 'hilarity',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'history',\n",
       " 'home',\n",
       " 'honestly',\n",
       " 'hong',\n",
       " 'horrifying',\n",
       " 'horror',\n",
       " 'horton',\n",
       " 'host',\n",
       " 'house',\n",
       " 'how',\n",
       " 'hurt',\n",
       " 'ice',\n",
       " 'if',\n",
       " 'impeccable',\n",
       " 'importance',\n",
       " 'impossible',\n",
       " 'impressed',\n",
       " 'in',\n",
       " 'incoherence',\n",
       " 'indecipherable',\n",
       " 'independent',\n",
       " 'indication',\n",
       " 'indie',\n",
       " 'inducing',\n",
       " 'indulgent',\n",
       " 'inept',\n",
       " 'inoffensive',\n",
       " 'inspired',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'into',\n",
       " 'intrigue',\n",
       " 'introspective',\n",
       " 'irish',\n",
       " 'is',\n",
       " 'ismail',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'james',\n",
       " 'jaunt',\n",
       " 'jokes',\n",
       " 'jones',\n",
       " 'joy',\n",
       " 'judge',\n",
       " 'judgment',\n",
       " 'juicy',\n",
       " 'just',\n",
       " 'keeps',\n",
       " 'know',\n",
       " 'kong',\n",
       " 'kooky',\n",
       " 'kung',\n",
       " 'lack',\n",
       " 'largely',\n",
       " 'latest',\n",
       " 'lead',\n",
       " 'leave',\n",
       " 'less',\n",
       " 'letting',\n",
       " 'lie',\n",
       " 'life',\n",
       " 'like',\n",
       " 'line',\n",
       " 'little',\n",
       " 'lives',\n",
       " 'looking',\n",
       " 'love',\n",
       " 'lrb',\n",
       " 'lunatic',\n",
       " 'made',\n",
       " 'mainland',\n",
       " 'make',\n",
       " 'makers',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'mamet',\n",
       " 'man',\n",
       " 'manipulative',\n",
       " 'many',\n",
       " 'mark',\n",
       " 'martial',\n",
       " 'material',\n",
       " 'meaning',\n",
       " 'means',\n",
       " 'meaty',\n",
       " 'memorable',\n",
       " 'merchant',\n",
       " 'mess',\n",
       " 'middle',\n",
       " 'midlife',\n",
       " 'midnight',\n",
       " 'might',\n",
       " 'mile',\n",
       " 'mill',\n",
       " 'minded',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miramax',\n",
       " 'modern',\n",
       " 'modest',\n",
       " 'moment',\n",
       " 'mongrel',\n",
       " 'monstrous',\n",
       " 'monty',\n",
       " 'mood',\n",
       " 'moonlight',\n",
       " 'more',\n",
       " 'most',\n",
       " 'movements',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'mr',\n",
       " 'much',\n",
       " 'murder',\n",
       " 'myth',\n",
       " 'mythic',\n",
       " 'narrative',\n",
       " 'narratively',\n",
       " 'nearly',\n",
       " 'nerve',\n",
       " 'new',\n",
       " 'no',\n",
       " 'none',\n",
       " 'norris',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'occasional',\n",
       " 'occasionally',\n",
       " 'occurs',\n",
       " 'oddest',\n",
       " 'oedekerk',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offering',\n",
       " 'offers',\n",
       " 'on',\n",
       " 'one',\n",
       " 'only',\n",
       " 'opera',\n",
       " 'opportunities',\n",
       " 'option',\n",
       " 'or',\n",
       " 'oscar',\n",
       " 'other',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'out',\n",
       " 'overeager',\n",
       " 'overexposed',\n",
       " 'pacing',\n",
       " 'pages',\n",
       " 'paralyzed',\n",
       " 'particularly',\n",
       " 'party',\n",
       " 'pat',\n",
       " 'path',\n",
       " 'pedigree',\n",
       " 'pellington',\n",
       " 'pep',\n",
       " 'peppering',\n",
       " 'performance',\n",
       " 'performances',\n",
       " 'perhaps',\n",
       " 'perspective',\n",
       " 'perverse',\n",
       " 'peter',\n",
       " 'pg',\n",
       " 'phrase',\n",
       " 'place',\n",
       " 'plausible',\n",
       " 'playing',\n",
       " 'plays',\n",
       " 'playwright',\n",
       " 'plodding',\n",
       " 'plot',\n",
       " 'poet',\n",
       " 'poetry',\n",
       " 'point',\n",
       " 'political',\n",
       " 'poorly',\n",
       " 'pop',\n",
       " 'positively',\n",
       " 'pow',\n",
       " 'powers',\n",
       " 'preach',\n",
       " 'prevention',\n",
       " 'primary',\n",
       " 'processed',\n",
       " 'progression',\n",
       " 'proportions',\n",
       " 'proves',\n",
       " 'provocative',\n",
       " 'publishing',\n",
       " 'purists',\n",
       " 'quiet',\n",
       " 'quotations',\n",
       " 'rambling',\n",
       " 'ramifications',\n",
       " 'rather',\n",
       " 'rating',\n",
       " 'rattling',\n",
       " 're',\n",
       " 'reading',\n",
       " 'realization',\n",
       " 'reason',\n",
       " 'recommend',\n",
       " 'reek',\n",
       " 'reeks',\n",
       " 'refreshing',\n",
       " 'reigen',\n",
       " 'relationship',\n",
       " 'relic',\n",
       " 'relief',\n",
       " 'remain',\n",
       " 'remakes',\n",
       " 'reminders',\n",
       " 'reno',\n",
       " 'rest',\n",
       " 'rewrite',\n",
       " 'ride',\n",
       " 'right',\n",
       " 'role',\n",
       " 'roll',\n",
       " 'romantic',\n",
       " 'romantics',\n",
       " 'rooted',\n",
       " 'rot',\n",
       " 'rrb',\n",
       " 'run',\n",
       " 'runaway',\n",
       " 'rut',\n",
       " 'same',\n",
       " 'sanguine',\n",
       " 'satire',\n",
       " 'satisfying',\n",
       " 'say',\n",
       " 'scenes',\n",
       " 'scherfig',\n",
       " 'schnitzler',\n",
       " 'sci',\n",
       " 'screen',\n",
       " 'screenplay',\n",
       " 'script',\n",
       " 'see',\n",
       " 'seeking',\n",
       " 'self',\n",
       " 'sensational',\n",
       " 'sense',\n",
       " 'series',\n",
       " 'serious',\n",
       " 'serve',\n",
       " 'setting',\n",
       " 'sex',\n",
       " 'shakespeare',\n",
       " 'shakespearean',\n",
       " 'shell',\n",
       " 'shelves',\n",
       " 'shiver',\n",
       " 'shocker',\n",
       " 'should',\n",
       " 'show',\n",
       " 'silly',\n",
       " 'sincere',\n",
       " 'sitting',\n",
       " 'size',\n",
       " 'skin',\n",
       " 'skittish',\n",
       " 'sleeping',\n",
       " 'slip',\n",
       " 'smart',\n",
       " 'smiles',\n",
       " 'snow',\n",
       " 'so',\n",
       " 'soap',\n",
       " 'some',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'soon',\n",
       " 'sounding',\n",
       " 'source',\n",
       " 'spectacularly',\n",
       " 'spooky',\n",
       " 'start',\n",
       " 'still',\n",
       " 'stoner',\n",
       " 'story',\n",
       " 'storytelling',\n",
       " 'strong',\n",
       " 'structure',\n",
       " 'struggle',\n",
       " 'study',\n",
       " 'stumble',\n",
       " 'subject',\n",
       " 'subjects',\n",
       " 'substitutable',\n",
       " 'subtly',\n",
       " 'success',\n",
       " 'sunday',\n",
       " 'sure',\n",
       " 'suspect',\n",
       " 'swaying',\n",
       " 'sweet',\n",
       " 'tackled',\n",
       " 'takes',\n",
       " 'tartakovsky',\n",
       " 'team',\n",
       " 'teeth',\n",
       " 'tender',\n",
       " 'tension',\n",
       " 'terms',\n",
       " 'terror',\n",
       " 'than',\n",
       " 'thanks',\n",
       " 'that',\n",
       " 'the',\n",
       " 'theater',\n",
       " 'their',\n",
       " 'then',\n",
       " 'there',\n",
       " 'they',\n",
       " 'thick',\n",
       " 'things',\n",
       " 'thinking',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thriller',\n",
       " 'thrilling',\n",
       " 'through',\n",
       " 'throw',\n",
       " 'ties',\n",
       " 'time',\n",
       " 'times',\n",
       " 'title',\n",
       " 'to',\n",
       " 'too',\n",
       " 'totally',\n",
       " 'tragedies',\n",
       " 'tragedy',\n",
       " 'transforms',\n",
       " 'transparent',\n",
       " 'tricky',\n",
       " 'trouble',\n",
       " 'true',\n",
       " 'try',\n",
       " 'tv',\n",
       " 'two',\n",
       " 'ultimately',\n",
       " 'under',\n",
       " 'undergoing',\n",
       " 'unexpected',\n",
       " 'unless',\n",
       " 'unnamed',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'usual',\n",
       " 'vapid',\n",
       " 'vaudeville',\n",
       " 'veiling',\n",
       " 'very',\n",
       " 'vignettes',\n",
       " 'villains',\n",
       " 'vincent',\n",
       " 'visual',\n",
       " 'waking',\n",
       " 'war',\n",
       " 'waste',\n",
       " 'watching',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'weirdo',\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'west',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whitewash',\n",
       " 'who',\n",
       " 'wild',\n",
       " 'wilde',\n",
       " 'will',\n",
       " 'williams',\n",
       " 'willingness',\n",
       " 'windtalkers',\n",
       " 'winning',\n",
       " 'wit',\n",
       " 'with',\n",
       " 'woman',\n",
       " 'wondering',\n",
       " 'wong',\n",
       " 'work',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'writers',\n",
       " 'year',\n",
       " 'york',\n",
       " 'you',\n",
       " 'young',\n",
       " 'your',\n",
       " 'youth',\n",
       " 'zingers']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#提取词袋模型,用countvectorize会产生词频统计\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x673 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8987 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC( probability=True)\n",
    "clf.fit(X_train_vec,spres[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(X_train_vec[1021]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print(spres[1021])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上课笔记 0425\n",
    "误差向后传播\n",
    "+ 前馈神经网络\n",
    "+ 卷积神经网络\n",
    "+ 循环神经网络\n",
    "\n",
    "感知机不能解决异或问题\n",
    "\n",
    "分段学习到端对端学习\n",
    "\n",
    "+ 神经元 mcp模型\n",
    "\n",
    "给定n个二值化的输入数据与连接参数 mcp神经元模型对输入数据线性加权求和，然后用函数将加权$\\phi(x)$累加结果映射为0或1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task2 用cnn、lstm实现文本分类任务\n",
    "+ 要求同task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    \"\"\"文本分类，RNN模型\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        # 三个待输入的数据\n",
    "        self.embedding = nn.Embedding(5000, 64)  # 进行词嵌入\n",
    "        self.rnn = nn.LSTM(input_size=64, hidden_size=128, bidirectional=True)\n",
    "        # self.rnn = nn.GRU(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        self.f1 = nn.Sequential(nn.Linear(256, 10),\n",
    "                                nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # batch_size x text_len x embedding_size 64*600*64\n",
    "        x= x.permute(1, 0, 2) # text_len x batch_size x embedding_size 600*64*64\n",
    "        x, (h_n, c_n)= self.rnn(x) #x为600*64*256, h_n为2*64*128 lstm_out       Sentence_length * Batch_size * (hidden_layers * 2 [bio-direct]) h_n           （num_layers * 2） * Batch_size * hidden_layers\n",
    "        final_feature_map = F.dropout(h_n, 0.8)\n",
    "        feature_map = torch.cat([final_feature_map[i, :, :] for i in range(final_feature_map.shape[0])], dim=1) #64*256 Batch_size * (hidden_size * hidden_layers * 2)\n",
    "        final_out = self.f1(feature_map) #64*10 batch_size * class_num\n",
    "        return final_out\n",
    "\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(5000, 64)\n",
    "        self.conv = nn.Sequential(nn.Conv1d(in_channels=64,\n",
    "                                        out_channels=256,\n",
    "                                        kernel_size=5),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool1d(kernel_size=596))\n",
    "\n",
    "        self.f1 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # batch_size x text_len x embedding_size 64*600*64\n",
    "        x = x.permute(0, 2, 1) #64*64*600\n",
    "\n",
    "        x = self.conv(x)  #Conv1后64*256*596,ReLU后不变,NaxPool1d后64*256*1\n",
    "\n",
    "        x = x.view(-1, x.size(1)) #64*256\n",
    "        x = F.dropout(x, 0.8)\n",
    "        x = self.f1(x)    #64*10 batch_size * class_num\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习里的Embedding\n",
    "这个概念在深度学习领域最原初的切入点是所谓的Manifold Hypothesis（流形假设）。流形假设是指“**自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间**”。那么，深度学习的任务就是把高维原始数据（图像，句子）映射到低维流形，使得高维的原始数据被映射到低维流形之后变得可分，而这个映射就叫嵌入（Embedding）。比如Word Embedding，就是把单词组成的句子映射到一个表征向量。但后来不知咋回事，开始把低维流形的表征向量叫做Embedding，其实是一种误用。。。\n",
    "\n",
    "作者：刘斯坦\n",
    "链接：https://www.zhihu.com/question/38002635/answer/1382442522\n",
    "来源：知乎\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "\n",
    "## 理解神经网络中的Dropout\n",
    "dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。\n",
    "\n",
    "过拟合是深度神经网（DNN）中的一个常见问题：模型只学会在训练集上分类，这些年提出的许多过拟合问题的解决方案，其中dropout具有简单性而且效果也非常良好。\n",
    "\n",
    "\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「SuPhoebe」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/u013007900/article/details/78120669/\n",
    "## PyTorch 中 LSTM 的 output、h_n 和 c_n 之间的关系\n",
    "+ h_n：最后一个时间步的输出，即 h_n = output[:, -1, :]（一般可以直接输入到后续的全连接层，在 Keras 中通过设置参数 return_sequences=False 获得）\n",
    "+ c_n：最后一个时间步 LSTM cell 的状态（一般用不到）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练集文件\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras as kr\n",
    "import os\n",
    "\n",
    "try:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "    is_py3 = False\n",
    "except NameError:\n",
    "    is_py3 = True\n",
    "\n",
    "\n",
    "def native_content(content):\n",
    "    if not is_py3:\n",
    "        return content.decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "\n",
    "def open_file(filename, mode='r'):\n",
    "    return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(native_content(content)))\n",
    "                    labels.append(native_content(label))\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n",
    "\n",
    "\n",
    "def build_vocab(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file(train_dir)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)  #统计最常出现的字\n",
    "    # print(count_pairs)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # print(words)\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')\n",
    "\n",
    "\n",
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    with open_file(vocab_dir) as fp:\n",
    "        # 如果是py2 则每个值都转化为unicode\n",
    "        words = [native_content(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "\n",
    "    categories = [native_content(x) for x in categories]\n",
    "\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "    return categories, cat_to_id\n",
    "\n",
    "\n",
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return ''.join(words[x] for x in content)\n",
    "\n",
    "\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示\"\"\"\n",
    "    contents, labels = read_file(filename)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return x_pad, y_pad\n",
    "\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    # print('inter')\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    # print(indices)\n",
    "    x_shuffle = x[indices]\n",
    "    # print(x_shuffle)\n",
    "    y_shuffle = y[indices]\n",
    "    # print(y_shuffle)\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pad_sequences序列预处理\n",
    "``pad_sequences = tf.contrib.keras.preprocessing.sequence.pad_sequences``\n",
    "\n",
    "``keras.preprocessing.sequence.pad_sequences(sequences,maxlen=None,dtype='int32',padding='pre',truncating='pre', value=0.)``\n",
    "\n",
    "解释：\n",
    "\n",
    "sequences：浮点数或整数构成的两层嵌套列表\n",
    "\n",
    "maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.在命名实体识别任务中，主要是指句子的最大长度\n",
    "\n",
    "dtype：返回的numpy array的数据类型\n",
    "\n",
    "padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "\n",
    "truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "\n",
    "value：浮点数，此值将在填充时代替默认的填充值0\n",
    "\n",
    " \n",
    "\n",
    "返回形如(nb_samples,nb_timesteps)的2D张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=read_file(\"G:\\cnews\\cnews.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_vocab(\"G:\\cnews\\cnews.train.txt\",\"G:\\cnews\\cnews.vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "def train():\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id,600)#获取训练数据每个字的id和对应标签的one-hot形式\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id,600)\n",
    "    #使用LSTM或者CNN\n",
    "    model = TextRNN()\n",
    "    # model = TextCNN()\n",
    "    #选择损失函数\n",
    "    #Loss = nn.MultiLabelSoftMarginLoss()\n",
    "    # Loss = nn.BCELoss()\n",
    "    Loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(100):\n",
    "        i = 0\n",
    "        print('epoch:{}'.format(epoch))\n",
    "        batch_train = batch_iter(x_train, y_train,64)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            i +=1\n",
    "            # print(i)\n",
    "            x = np.array(x_batch)\n",
    "            y = np.array(y_batch)\n",
    "            x = torch.LongTensor(x)\n",
    "            y = torch.Tensor(y)\n",
    "            # y = torch.LongTensor(y)\n",
    "            # x = Variable(x)\n",
    "            # y = Variable(y)\n",
    "            out = model(x)\n",
    "            loss = Loss(out,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 对模型进行验证\n",
    "            if i % 90 == 0:\n",
    "                los, accracy = evaluate(model, Loss, x_val, y_val)\n",
    "                print('loss:{},accracy:{}'.format(los, accracy))\n",
    "                if accracy > best_val_acc:\n",
    "                    torch.save(model.state_dict(), 'model_params.pkl')\n",
    "                    best_val_acc = accracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'G:\\cnews'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, Loss, x_val, y_val):\n",
    "    \"\"\"测试集上准确率评估\"\"\"\n",
    "    batch_val = batch_iter(x_val, y_val, 64)\n",
    "    acc = 0\n",
    "    los = 0\n",
    "    for x_batch, y_batch in batch_val:\n",
    "        size = len(x_batch)\n",
    "        x = np.array(x_batch)\n",
    "        y = np.array(y_batch)\n",
    "        x = torch.LongTensor(x)\n",
    "        y = torch.Tensor(y)\n",
    "        # y = torch.LongTensor(y)\n",
    "        # x = Variable(x)\n",
    "        # y = Variable(y)\n",
    "        out = model(x)\n",
    "        loss = Loss(out, y)\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        loss_value = np.mean(loss.detach().numpy())\n",
    "        accracy = np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).numpy())\n",
    "        acc +=accracy*size\n",
    "        los +=loss_value*size\n",
    "    return los/len(x_val), acc/len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.07750285989046096,accracy:0.3084\n",
      "loss:0.07362846817970276,accracy:0.3564\n",
      "loss:0.07446694656610489,accracy:0.3776\n",
      "loss:0.06720132167339325,accracy:0.4234\n",
      "loss:0.06628206780552864,accracy:0.4504\n",
      "loss:0.06595048649311065,accracy:0.4586\n"
     ]
    }
   ],
   "source": [
    "categories, cat_to_id = read_category()\n",
    "    #获取训练文本中所有出现过的字及其所对应的id\n",
    "words, word_to_id = read_vocab(vocab_dir)\n",
    "    #获取字数\n",
    "vocab_size = len(words)\n",
    "print('train')\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jianshu.com/p/8acb163dd1fe\n",
    "### python中collection的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵损失函数\n",
    "$\\hat{y}$代表模型预测类别分布概率\n",
    "+ 对x而言 y和$\\hat{y}$的交叉熵损失函数定义为$y×{log(\\hat{y})}$\n",
    "感知机网络\n",
    "+ 没有隐藏层\n",
    "+ 无法拟合复杂数据\n",
    "从标注数据除法优化模型参数\n",
    "\n",
    "# 上课笔记 0427\n",
    "## bp算法\n",
    "将误差从后向前传递，更新参数，将误差分摊给各层所有单元，得到各层单元所产生的误差，根据这个误差让各层单元夫妻各自责任、修正各单元参数。\n",
    "\n",
    "误差反向传播例子：\n",
    "$$f(w,x)=\\frac{1}{1+e^{-(w_{0}x_{0}+w_{1}x_{1}+w_{2})}}$$\n",
    "反向传播：\n",
    "+ 梯度计算\n",
    "+ 更新参数\n",
    "\n",
    "机器学习的能力在于拟合和优化\n",
    "$$ w_{j}^{new}=w_{j}^{old}-\\theta×\\frac{\\partial{Model_{loss}}}{\\partial{w_{j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从BP神经网络到循环神经网络RNN再到LSTM\n",
    "BP神经网络是线性权重的激活函数模型，即输入一个向量，对向量进行加权处理后输入到隐含层神经元的激活函数当中去，再将函数的输出值进行加权处理最后得到输出层的值。\n",
    "\n",
    "根据输出层和真实值的关系，我们可以做一个误差函数（用广义的名词来说可以使距离），一般误差函数使用的是均方误差。假设为L。\n",
    "\n",
    "BP神经网络要优化的参数有不同层之间的权重U，$w_i$  ，V，L是这些权重参数的函数。我们利用L对U，$w_i$ ，V的梯度来对其进行更新，直到L函数不再变动。\n",
    "\n",
    "RNN（Recurrent Neural Network）是一类用于处理序列数据的神经网络。\n",
    "\n",
    "基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4.], requires_grad=True)\n",
      "tensor([ 512.,  768., 1024.], grad_fn=<MulBackward0>)\n",
      "tensor([256., 256., 256.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([2, 3, 4], dtype=torch.float, requires_grad=True)\n",
    "print(x)\n",
    "y = x * 2\n",
    "while y.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)\n",
    "\n",
    "y.backward(torch.ones_like(y))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 12, 30])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,3,5])*np.array([2,4,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播算法\n",
    "假设训练集只有一个实例$(x^{(1)},y^{(1)})$，神经网络是一个四层的神经网络，其中$K=4,S_L=4,L=4$：\n",
    "\n",
    "前向传播算法： \n",
    " \n",
    " <img src='https://i.loli.net/2018/12/02/5c03dbc252db6.png' width=500>\n",
    " \n",
    "我们从最后一层的误差开始计算，误差是激活单元的预测$(a^{(4)}_k)$与实际值$(y^k)$之间的误差，$(k=1:K)$。     \n",
    "\n",
    "1. 第四层：用δ来表示误差，则：$\\delta^{(4)} = a^{(4)} -y$\n",
    " \n",
    "2. 第三层：利用$\\delta^{(4)}$来计算前一层的误差：$\\delta^{(3)} = (\\theta^{(3)})^T\\delta^{(4)} * g^`(z^{(3)})$     \n",
    " 其中$g^`(z^{(3)})$是 S 形函数的导数，$g^`(z^{(3)})=a^{(3)}*(1-a^{(3)})$。而 $(\\theta^{(3)})^T\\delta^{(4)} $则是权重导致的误差的和。\n",
    " \n",
    "3. 继续计算第二层的误差：$\\delta^{(2)} = (\\theta^{(2)})^T\\delta^{(3)} * g^`(z^{(2)})$ \n",
    "4. 第一层是输入变量，不存在误差。\n",
    "\n",
    "有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设 $\\lambda=0$，即我们不做任何正则化处理时有：\n",
    "$$\\frac{\\partial}{\\partial\\theta^{(l)}_{ij}}j(\\theta) = a^{(l)}_{j}\\delta^{l+1}_i$$\n",
    "\n",
    "**重要的是清楚地知道上面式子中上下标的含义：**\n",
    "\n",
    "+ l 代表目前所计算的是第几层 \n",
    "+ j 代表目前计算层中的激活单元的下标，也将是下一层的第 j 个输入变量的下标。 \n",
    "+ i 代表下一层中误差单元的下标，是受到权重矩阵中第 i 行影响的下一层中的误差单元的下标。 \n",
    "\n",
    "如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用$\\Delta^{(l)}_{ij}$来表示这个误差矩阵。第 l 层的第 i 个激活单元受到第 j 个参数影响而导致的误差。 \n",
    "\n",
    "我们的算法表示为： \n",
    " \n",
    "  <img src='https://i.loli.net/2018/12/02/5c03df8488f60.png' width=500>\n",
    " \n",
    "即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。\n",
    " \n",
    "在求出了$D^{(l)}_{ij}$之后，我们便可以计算代价函数的偏导数了，计算方法如下： \n",
    "\n",
    " $$\\Delta^{(l)}_{ij} := \\frac{1}{m}\\Delta^{(l)}_{ij}+\\lambda\\theta^{(l)}_{ij} if j\\neq0$$\n",
    " $$\\Delta^{(l)}_{ij} := \\frac{1}{m}\\Delta^{(l)}_{ij}+ if j=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a40ad87a603b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_relu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'D_in' is not defined"
     ]
    }
   ],
   "source": [
    "#https://blog.csdn.net/lee813/article/details/89609691介绍训练的一个blog\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "def train():\n",
    "    for t in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x)\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y) # 计算损失函数\n",
    "        print(t, loss.item())\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad() # 梯度置零，因为反向传播过程中梯度会累加上一次循环的梯度\n",
    "        loss.backward() # loss反向传播\n",
    "        optimizer.step() # 反向传播后参数更新 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 上面一段代码介绍了用`pytorch`训练模型的最基本的使用\n",
    "+ 在模型编写时候要重写网络结构，编写前向反馈函数\n",
    "+ 选择损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
